-------------------------------------------------------------
K8S Orchestration tech (manage containers, cluster of containers) = Docker Swarm, Kubernetes, MESOS from Apache

Kubernetes Helm = package manager
  
Playground - https://labs.play-with-k8s.com/

K8S architecture:

* Master Node = it is responsible for running the control plane
components :
- API server (provides REST API to talk with the cluster)
- etcd distributed cluster store (cluster state and configuration management)
- scheduler (schedule work to different workers)
- controller manager (daemonized control loop which supervises cluster state,  desired state == current state)

* Worker Node
components :
- kubelet (process receives requests from API server to run containers and watch them on local node)
- kube-proxy (process that manages networking rules, provides unique IP to pods)
- container engine (docker is used for container runtime)


K8S object model:

- we provide an object definition via .yml which is converted by kubectl ina JSON payload and sent to the API server

- we declare the K8S desired state using SPEC field and K8S system manages the STATUS field for objects
- at any point in time the K8S Control Plane tries to match the objects desired state with actual state (SPEC == STATUS field)
- to create an object, we need to provide the spec field to k8s API server

Type of objects:

* Pod = single instance of app (k8s object), logical collection of containers which share the same network space and mount the same volume

* Controller: Deployments, ReplicaSets 
    * ReplicationController = is a part of the master controller manager, that manages the no of PODS. Only equality-based selectors
    * ReplicaSet(next-gen) = support equality-based and set based selectors
    * Deployment = is a part of the master controller manager, declarative update to Pods and ReplicaSets for rolling update fashion

* Namespace = provide a way to divide cluster resources (partition cluster into sub-clusters)

* Labels {k:v} are attached to objects, but the do not provide uniqueness to objects, the labels are used only to organise and select objects
  e.g: {app:frontend, env:dev}

* Selectors : Equality-Based (== or !=) and Set-based (in, notin)

* Service = provide load balancing for pods



Each request to API Server, goes through 3 stages:

*Authentification = log in user

*Authorization = authorize API requests

*Admission Control = Software modules that can modify or reject the requests based on some additional checks, like Quota



Minikube (allinone: runs a single node kubernetes cluster inside a VM has the kubelet ,kube-apiserver,etcd,node-controller)
minikube version
minikube start
Pre-requisies for Minikube: https://kubernetes.io/docs/tasks/tools/install-minikube/


Kubernetes CLI:

$ kubectl get namespaces ( two default Namespaces: kube-system and default - objects of any other Namespace besides the ones created by K8S system)
$ kubectl  config get-contexts
$ kubectl cluster-info
$ kubectl get nodes
$ kubectl get pods
$ kubectl run hello-minikube 
$ kubectl get deployments
$ kubectl config view
$ kubectl get services (  Service can be exposed by type in ServiceSpec--type NodePort, Cluster IP, LoadBalancer

Chapter9: Services and kube-proxy

- Service = logically group pods and a policy to access them, due to the fact that the Pods are ephemeral (a new pod will have a new IP address) . The grouping is achieved by using Labels and Selectors
- kind: Service; apiVersion: v1; metadata: name: frontend-svc; spec: selector: app: frontend
- we are creating a frontend-svc Service by selecting all the Pods that have the Label app set to the frontend. The services gets an IP address 172.17.0.4 aka Cluster IP
- the Service also does load balancing 
- ServiceType: CluserIP (default service type, the service gets VirtualIP) or NodePort (VirtualIP + port useful for external access) or LoadBalancer or ExternalIP (route to one or more worker nodes)


- Kube-proxy = daemon which runs on workers and manages the IP-tables .

- Service discovery (done at runtime)
   * ENV variables
   * DNS : my-svc.my-namespace.svc.cluster.local

Openshift:

! PODS can talk between each other using ingress

Openshift objects:

Project = based on Namespace k8s object , in order to provide grouping functionality to resources
DeploymentConfig = based on Deployment k8s object
Template = set of objects that can be parameterized and processed to produce a list of objects


Build Strategies:

1) source  = From service catalog select a builder image (with S2I toolkit that produces a docker image and Python) + in the wizard paste the link of your repo

2) DockerImage = 

------------------------------------------------------------------------
DOCKER DOCKER DOCKER DOCKER DOCKER DOCKER

QA: 

ADD vs COPY in Dockerfile = ADD allows  to extract a local tar file into a specific directory in your Docker image.

Docker Multistage =  you use multiple FROM statements in your Dockerfile

Copy files in/from container = ?

Containers = based on linux namespaces and cgroups

Docker needs root permissions: sudo usermod -aG docker $(whoami)

Container  = isolated running process   
Image =  binary that includes all of the requirements for running a single Docker container, as well as metadata describing its needs and capabilities.
Volume = shared folder (for data persistence) , they are initialized when a container is created

#docker pull IMAGE:TAG
#docker pull dejanualex/pythonwebapp:part2


#docker run --name centa -it IMAGE bash (pull image and open the bash inside the container)
#docker run -p HOST_PORT:CONTAINER_PORT     IMAGE 
#docker run -it --entrypoint /bin/bash image  (overwrite entrypoint)
#docker run -e "SPRING_PROFILES_ACTIVE=dev" -e "ibs.redirectUri=www.example.com"  4d91e2372e9b (environment variables can be defined when you launch the container)

#docker run --detach --name jan --publish 127.0.0.1:6379:6379 redis (by default the it is mapped to broadcast 0.0.0.0)

If you do not know on which host port a container is exposed simply just:
#docker port redisDynamic 6379 (docker port container_id container_port , knowing that redis starts on 6379)

--name = provide name for container
--publish
-i            = Keep STDIN open even if not attached
-d = detach in the background (aka daemon)
-s = attach
--rm = automatically removes the container when the process exists

#docker start -ai CONTAINER (start a stopped container)
#docker exec -it CONTAINER (run command in a running container)
#docker exec  CONTAINER command
#docker exec -d -w /temp ubuntu_bash touch my_file.sh
#docker exec CONTAINER cat /etc/hostname (/bin/echo 'Hello world')


-a         = Attach to `STDIN`, `STDOUT` and/or `STDERR`
-t          =  Allocate a pseudo-tty
-d         = detach 
-w         = specify the working dir

Binding directories (volumes) is done using the option -v <host-dir>:<container-dir>

# docker run -p 80:8080 -v /opt/jenkins_data:/var/jenkins_home jenkins
# docker run -p 80:8080 -v $PWD:/var/jenkins_home jenkins



Build images 
docker build -t tag_name


--sig-proxy=true: Proxy all received signals to the process (non-TTY mode only)

Inspect Containers
#docker inspect --format '{{ .NetworkSettings.IPAddress }}' CONTAINER_NAME
#docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' CONTAINER_NAME
#docker logs -f CONTAINER_NAME (follow logs of container tail -f)

Inspect container mounts:
#docker inspect -f "{{.Mounts}}" container_id

Inspect container:
# docker ps --format '{{.Names}} is using {{.Image}}'

REMOVE or STOP containers:

#docker stop $(docker ps -q)  //stop all container using their ids
#docker rm -f $(docker ps -aq -f  status=exited) //remove all stopped containers . 

When you install Docker, it creates three networks automatically â€“ Bridge (default), Null and Host. 

 If you would like to attach the container with any other network specify the network information using the network command line parameter .

#docker run --network=host ubuntu
#docker network ls
#docker network create backend-network 
#docker network connect backend-network redis


